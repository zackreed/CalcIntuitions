\documentclass{ximera}

\title{Multivariable Optimization}
\author{Zack Reed}

\begin{document}
\begin{abstract}
In this activity we explore quadratic approximations of functions of multiple variables, introducing the Hessian matrix and the second derivative test for classifying critical points.
\end{abstract}
\maketitle


\section*{Quadratic Approximations of Functions: The Hessian Matrix and the Second Derivative Test}

Just as $f''(x)$ tells us about concavity in single-variable calculus, the \textbf{Hessian matrix} captures second-order information in multiple variables.

\subsection*{Defining the Hessian}

\begin{definition}
For a function $f(x,y)$, the \textbf{Hessian matrix} is:
$$H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}$$

The entries are:
\begin{itemize}
    \item $f_{xx}$: second partial derivative with respect to $x$
    \item $f_{yy}$: second partial derivative with respect to $y$
    \item $f_{xy}$ and $f_{yx}$: mixed partial derivatives (usually equal)
\end{itemize}
\end{definition}

\begin{problem}
Let's compute the Hessian of $f(x,y) = x^2 + y^2$.

First derivatives: $f_x = 2x$ and $f_y = 2y$

Second derivatives:
$$f_{xx} = \frac{\partial}{\partial x}(2x) = \answer{2}$$
$$f_{yy} = \frac{\partial}{\partial y}(2y) = \answer{2}$$
$$f_{xy} = \frac{\partial}{\partial y}(2x) = \answer{0}$$

The Hessian matrix is:
$$H = \begin{bmatrix} \answer{2} & \answer{0} \\ \answer{0} & \answer{2} \end{bmatrix}$$

\begin{feedback}
    Notice that this Hessian is constant - it doesn't depend on $(x,y)$! This happens for quadratic functions.
\end{feedback}
\end{problem}

\begin{problem}
Compute the Hessian of $f(x,y) = x^3 + y^3 - 3xy$.

First derivatives: $f_x = 3x^2 - 3y$ and $f_y = 3y^2 - 3x$

Second derivatives:
$$f_{xx} = \answer{6x}$$
$$f_{yy} = \answer{6y}$$
$$f_{xy} = \answer{-3}$$

The Hessian matrix is:
$$H = \begin{bmatrix} 6x & -3 \\ -3 & 6y \end{bmatrix}$$

At the critical point $(1,1)$:
$$H(1,1) = \begin{bmatrix} \answer{6} & \answer{-3} \\ \answer{-3} & \answer{6} \end{bmatrix}$$

\begin{feedback}
    Unlike the previous example, this Hessian changes depending on where we evaluate it. We'll need to evaluate at critical points to classify them.
\end{feedback}
\end{problem}

\subsection*{The Second Derivative Test}

\begin{definition}
Let $(x_0, y_0)$ be a critical point and let $H$ be the Hessian evaluated at that point. Define:
$$D = \det(H) = f_{xx} f_{yy} - (f_{xy})^2$$

This is called the \textbf{discriminant}.

Then:
\begin{itemize}
    \item If $D > 0$ and $f_{xx} > 0$: \textbf{local minimum}
    \item If $D > 0$ and $f_{xx} < 0$: \textbf{local maximum}
    \item If $D < 0$: \textbf{saddle point}
    \item If $D = 0$: \textbf{test is inconclusive}
\end{itemize}
\end{definition}

\begin{problem}
Why does the sign of $f_{xx}$ matter when $D > 0$?

\begin{multipleChoice}
    \choice{It determines whether the function is differentiable}
    \choice[correct]{It tells us the concavity in the $x$-direction}
    \choice{It measures the steepness of the gradient}
    \choice{It doesn't actually matter}
\end{multipleChoice}

\begin{feedback}
    When $D > 0$, both eigenvalues of the Hessian have the same sign. If $f_{xx} > 0$, both are positive (curving up = minimum). If $f_{xx} < 0$, both are negative (curving down = maximum).
\end{feedback}
\end{problem}

\begin{problem}
Let's classify the critical point of $f(x,y) = x^2 + y^2 - 4x + 6y + 5$ at $(2, -3)$.

We found earlier that the Hessian is:
$$H = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}$$

The discriminant is:
$$D = (2)(2) - (0)^2 = \answer{4}$$

Since $D > 0$ and $f_{xx} = 2 > 0$, the critical point is a \wordChoice{\choice[correct]{local minimum}\choice{local maximum}\choice{saddle point}}.

The minimum value is:
$$f(2,-3) = 4 + 9 - 8 - 18 + 5 = \answer{-8}$$

\begin{feedback}
    This makes sense - the function is $(x-2)^2 + (y+3)^2 - 8$, a paraboloid with vertex at $(2,-3,-8)$.
\end{feedback}
\end{problem}

\begin{problem}
Classify the critical point of $f(x,y) = x^2 - y^2$ at $(0,0)$.

The Hessian is:
$$H = \begin{bmatrix} 2 & 0 \\ 0 & -2 \end{bmatrix}$$

The discriminant is:
$$D = (2)(-2) - (0)^2 = \answer{-4}$$

Since $D < 0$, the critical point is a \wordChoice{\choice{local minimum}\choice{local maximum}\choice[correct]{saddle point}}.

\begin{feedback}
    When $D < 0$, the Hessian has eigenvalues of opposite sign - the function curves up in one direction and down in another, creating a saddle shape.
\end{feedback}
\end{problem}

\begin{problem}
Now a complete example. Find and classify all critical points of:
$$f(x,y) = x^3 + y^3 - 3xy$$

\textbf{Step 1: Find critical points}

The gradient is $\nabla f = \langle 3x^2 - 3y, 3y^2 - 3x \rangle$

Setting both components to zero:
$$x^2 - y = 0 \Rightarrow y = x^2$$
$$y^2 - x = 0 \Rightarrow y^2 = x$$

Substituting: $(x^2)^2 = x \Rightarrow x^4 - x = 0 \Rightarrow x(x^3-1) = 0$

So $x = \answer{0}$ or $x = \answer{1}$

The critical points are: $(\answer{0}, \answer{0})$ and $(\answer{1}, \answer{1})$

\textbf{Step 2: Compute the Hessian}

$$H = \begin{bmatrix} 6x & -3 \\ -3 & 6y \end{bmatrix}$$

\textbf{Step 3: Classify $(0,0)$}

$$H(0,0) = \begin{bmatrix} 0 & -3 \\ -3 & 0 \end{bmatrix}$$

$$D = (0)(0) - (-3)^2 = \answer{-9}$$

Since $D < 0$, $(0,0)$ is a \wordChoice{\choice{local minimum}\choice{local maximum}\choice[correct]{saddle point}}.

\textbf{Step 4: Classify $(1,1)$}

$$H(1,1) = \begin{bmatrix} 6 & -3 \\ -3 & 6 \end{bmatrix}$$

$$D = (6)(6) - (-3)^2 = 36 - 9 = \answer{27}$$

Since $D > 0$ and $f_{xx} = 6 > 0$, $(1,1)$ is a \wordChoice{\choice[correct]{local minimum}\choice{local maximum}\choice{saddle point}}.

The minimum value is: $f(1,1) = 1 + 1 - 3 = \answer{-1}$

\begin{feedback}
    This function has both a saddle point and a local minimum! Real-world optimization problems often have complex landscapes with multiple critical points.
\end{feedback}
\end{problem}

\end{document}
