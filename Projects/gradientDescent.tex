\documentclass{ximera}

\title{Warm up Project Help: Data Fitting with Gradient Descent}
\author{Zack Reed}

\begin{document}
\begin{abstract}
In this project we'll explore how to model data fitting problems using gradient descent. We'll begin with a simple linear regression model, then extend it to more complex models using calculus and optimization techniques.
\end{abstract}
\maketitle

\section*{Mini Project - Fitting Data with Gradient Descent}

\subsection*{Introduction}
Welcome to the "fitting data with gradient descent" mini project! This will serve as a warm up for your final project, and also features one of the possible topic options for the final project: gradient descent.

For this mini-project we'll explore how to use \textbf{gradient descent} to approximate data with polynomials. Using gradient descent to find the minimum error is a bit like using a sledgehammer on a nail, however the point of this project isn't to give you the \emph{best} way to fit data, it's to intuitively motivate gradient descent for the plethora of cases where it is the \emph{best} or \emph{only} way to find a minimum.

This uses optimization to solve the real-world problem of \textbf{model selection}: what function best represents observed data?

\section*{Modeling Data With Polynomials}
A basic data analysis technique in many fields is looking for patterns or trends in data that follow nice simple functions, such as polynomials. 

Consider the following data set that we want to analyze:

\begin{center}
\includegraphics{data_linear_initial.png}
\end{center}

Notice that the data points linearly decrease from around $(-2, 6)$ to $(2, -6)$.

One common way to explain data with simple models is to find curves (or surfaces) that ``fit''the data well. This means that the $(x,y)$ values of the data are reasonably ``close'' to the $(x,y)$ values of the model.

In this initial data, a simple decreasing linear function seems to fit the data well, such as the following line cutting through the data:

\begin{center}
\includegraphics{data_linear_initial_with_line.png}
\end{center}

\begin{example}
  Let's look at data that might be modeled by a quadratic function:

  \begin{center}
  \includegraphics{data_quadratic_initial_with_curve.png}
  \end{center}

  Again, we see that a simple quadratic function (a parabola) seems to fit the data well, meaning that the $(x,y)$ values of the data are reasonably ``close'' to the $(x,y)$ values of the model.
\end{example}

Let's look at one more example, this time with cubic data:

\begin{example}
  Consider the following data that might be modeled by a cubic function:

  \begin{center}
  \includegraphics{data_cubic_initial_with_curve.png}
  \end{center}

  Again, we see that a simple cubic function seems to fit the data well.
\end{example}

In all of these examples, we use simple polynomial functions to model data. There are various reasons why polynomials are good choices for this particular data, but we will leave that discussion for other statistics or data science courses.

In this mini project, you will learn a common way that gradients are used to learn the parameters of models. This particular example of polynomial fitting doesn't require the use of gradients, but it's a good introductory context that fits within the scope of this course.

\section*{Measuring Error: What does ``Close'' Mean?}

The way that we use gradients to learn good mdels occurs through \emph{optimization}: 

\begin{enumerate}
  \item We define a measure of ``error'' between the data and the model.
  \item We use optimization to find a model that minimizes the error.
\end{enumerate}

To do this, we first need to define what we mean by ``error''. One industry best practice for computing error is called \textbf{mean squared error (MSE)}. 

This calculates the vertical distances between the data heights ($\hat{y}_i$) and the model heights ($y_i$), squares the distances $({\hat{y}_i} - y_i)^2$, and then finds the average (mean) of those square distances:
\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n ({\hat{y}_i} - y_i)^2
\]

This entails:
\begin{enumerate}
  \item Finding the vertical distance between the points and the locations on the model.
  \item Squaring the differences
  \item Averaging the square differences.
\end{enumerate}

Let's take a look at some examples of the possible error between data and models.

\begin{example}
  Consider the following data and cubic model:
  
  \begin{center}
  \includegraphics{cubic_error_reading.png}
  \end{center}

  The vertical red lines represent the vertical distances between the data points and the model curve. 

  To calculate the MSE, we would square each of these vertical distances, sum them up, and then divide by the number of data points.

  In this particular case the cubic model is a \wordChoice{\choice{good}\choice[correct]{poor}} fit for the data. You can see especially \wordChoice{\choice[correct]{large}\choice{small}} vertical distances between the data points and the model curve at the ends of the plot, which would mean a \wordChoice{\choice{small}\choice[correct]{large}} MSE value.
\end{example}

\begin{example}
  Consider the following data and linear model:
  
  \begin{center}
  \includegraphics{linear_error_reading.png}
  \end{center}

  In this case, the linear model is a \wordChoice{\choice{good}\choice[correct]{poor}} fit for the data. You can see especially \wordChoice{\choice{small}\choice[correct]{large}} vertical distances between the data points and the model line at the \wordChoice{\choice{left end}\choice{right end}\choice[correct]{middle}} of the plot, which would mean a \wordChoice{\choice[correct]{large}\choice{small}} MSE value.

  A better model for this data would be a quadratic or cubic function.
\end{example}

\begin{example}
  Consider the following data and quadratic model:
  
  \begin{center}
  \includegraphics{quadratic_error_reading.png}
  \end{center}

  In this case, the quadratic model is a \wordChoice{\choice[correct]{good}\choice{poor}} fit for the data. You can see especially \wordChoice{\choice[correct]{small}\choice{large}} vertical distances between the data points and the model curve throughout the plot, which would mean a \wordChoice{\choice{small}\choice[correct]{large}} MSE value.
\end{example}

\section*{Minimizing Error: Gradient Descent}
A key property that makes $\text{MSE}$ very nice is that we can turn the coefficients of the polynomial models into variables, which can then be used as input variables for a minimization process.

For instance, consider the linear model $y = a_0 + a_1 x$. The coefficients $a_0$ and $a_1$ are typically fixed numbers, creating lines like $y = 2 + 3x$ or $y = -1 + 0.5x$.

If we let $a_0$ and $a_1$ be variables, then we can measure the error between the data and the model for any possible line $y = a_0 + a_1 x$. We'll use the notation $f(a_0, a_1)$ to represent the error between the data and the line $y = a_0 + a_1 x$.

\begin{remark}

We can then similarly create error functions for quadratic models $y = a_0 + a_1 x + a_2 x^2$ and cubic models $y = a_0 + a_1 x + a_2 x^2 + a_3 x^3$, where the error functions are $f(a_0, a_1, a_2)$ and $f(a_0, a_1, a_2, a_3)$ respectively.

\end{remark}

\begin{example}

For instance, the error from a linear approximation $y = a_0 + a_1 x$ is a 2-variable function from the data points $(x_i, y_i)$:
\[
f(a_0, a_1) = \frac{1}{n} \sum_{i=1}^n (y_i - (a_0 + a_1 x_i))^2
\]
The error from a quadratic approximation $y = a_0 + a_1 x + a_2 x^2$ is a 3-variable function from the data points $(x_i, y_i)$:
\[
f(a_0, a_1, a_2) = \frac{1}{n} \sum_{i=1}^n (y_i - (a_0 + a_1 x_i + a_2 x_i^2))^2
\]
Hopefully you can see that we can build a multivariable error function for any possible polynomial approximation, however ugly it might be.

Using the linear data from earlier, we get the following error function:

$$ 5.429a_1 - 0.4446a_0 + 0.01708a_0a_1 + a_0^2 + 1.26a_1^2 + 7.142$$

While it was obtained by many many calculations, we get a simple quadratic function in two variables $a_0$ and $a_1$. The plot looks like this:

\begin{center}
\includegraphics{loss_surface.png}
\end{center}

As you'd expect, the simple bowl shape means that it is quite easy to find a global minimum for this error function!

\end{example}


\section*{Gradient Descent}
Now that we have the error functions, we can use calculus to minimize the error!

Using calculus tools, this surface is actually quite easy to minimize, and in fact we're guaranteed a global minimum!
That being said, this also provides a nice easy introduction to gradient descent. 

With gradient descent, you randomly place initial possible input values (in this case $a_0$ and $a_1$) on the domain, and then imagine the points $f(a_0, a_1)$ rolling down the "hill" of the function surface until they reach the minimum.

\begin{example}

  Let's see an animation of this process unfolding for our linear data error function.

\youtube{87tJZ8xmvmY}

A number of points are placed randomly on the surface and moving towards the "bottom of the hill", the minimum value on the surface. 

The minimum for this particular error function is at the location $(x, y) = (0.239935, -2.154704)$.

This means that theoretically a good fitting linear model for the data is:

\[y = 0.239935 - 2.154704 x\]

That particular line, plotted against the data, looks like this:

\begin{center}
\includegraphics{linear_learned.png}
\end{center}

You can see a reasonably good fit for the data!

\end{example}

\begin{remark}
  Remember that gradients live in the \wordChoice{\choice{output}\choice[correct]{input}} space of functions. 

  So the gradient is the \wordChoice{\choice{output}\choice[correct]{input}} vector that points in the direction of steepest increase of the function. Meaning if we move the \wordChoice{\choice{output}\choice[correct]{input}} coordinates in the direction of the gradient, the function value will increase the fastest.
  
  For gradient descent, we take the negative gradient so that we move in the direction of steepest decrease of the function. By following the negative gradient, we eventually reach a local minimum of the function.

  If we were to follow the same gradient descent process as above, but looking at the domain of the function, we would see something like this:

\begin{center}
  \youtube{S7K3SM3aPas}
\end{center}

The arrow on each of the points in the animation is the gradient. At each update step in the gradient descent the points move in the direction of the negative gradient until they reach the minimum point.

We can also view the lines resulting from each of the $(a_0, a_1)$ pairs as the gradient descent process unfolds:

\begin{center}
  \youtube{UHWDeQPdhho}
\end{center}

Remember that each point $(a_0, a_1)$ represents a line $y = a_0 + a_1 x$. As the points move according to gradient descent, the lines rotate and shift until they reach the best fit line for the data!

\end{remark}

\subsection*{Polynomial Approximations: Gradient Descent}
For quadratics and cubics, we recreate this same process, but we need one variable for each coefficient of the polynomial. 

So for quadratics, we have three variables $(a_0, a_1, a_2)$ with the quadratic model $y = a_0 + a_1 x + a_2 x^2$, and for cubics we have four variables $(a_0, a_1, a_2, a_3)$ with the cubic model $y = a_0 + a_1 x + a_2 x^2 + a_3 x^3$.

\begin{example}
  First let's look at the quadratic data from earlier. 
  
  This three-variable error function is given by:

  $$2.472a_0 - 3.719a_1 + 5.173a_2 + 0.3782a_0a_1 + 3.294a_0a_2 + 1.568a_1a_2 + a_0^2 + 1.647a_1^2 + 4.092a_2^2 + 6.413$$
  
  We can no longer view the error function surface, but we can still see how the input points move in the 3D domain of the function:

\youtube{G6oZXdCgMlE}

As before, the points move according to the negative gradient until they reach the minimum point. In this case, the minimum is at the location $(x, y, z) = (0.0122, 1.5740, -0.9384)$. This makes a quadratic model for the data of:

\[y = 0.0122 + 1.5740 x - 0.9384 x^2\]

We can still view the resulting curves from each of the $(a_0, a_1, a_2)$ triplets as the gradient descent process unfolds:

\youtube{cXLt8zp4ztU}
\end{example}

\begin{example}
  Let's look at the cubic data from earlier. Here we can't view the output or input animations, but can still view the resulting curves from each of the $(a_0, a_1, a_2, a_3)$ quadruplets as the gradient descent process unfolds. 

  The error function is 

  $$2.531a_0a_2 - 0.9788a_1 - 11.98a_2 - 2.218a_3 - 0.6167*a_0*a_1 - 6.84a_0 - 1.259a_0a_3 - 1.259a_1a_2 + 5.882a_1a_3 - 3.309a_2a_3 + a_0^2 + 1.265a_1^2 + 2.941a_2^2 + 8.2a_3^2 + 18.21$$

  Notice that, even though it is somewhat complicated and has many terms, it is still a quadratic function in four variables, so it will be easily minimized by gradient descent!

  The following animation shows the resulting cubic curves slowly moving to fit the data as the gradient descent process unfolds:

\youtube{Tt___p75jhU}
\end{example}

\section*{Your Turn: Finding a Good Fit}
Now it's your turn! To finish out the warm up activity, you will generate your own data and analyze it!

The data will either be quadratic or cubic, and you have to decide which gives a better fit both by doing some gradient descent, but also by using the optimization tools from the book!

You'll want to follow the steps in the live script to recreate the same steps we did above, but with your own data! You'll also need to determine which model (linear, quadratic, or cubic) gives the best fit for your data (and why!).

Good luck, and have fun!

\end{document}
